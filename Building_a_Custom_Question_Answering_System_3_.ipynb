{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "v2vx13ysgHum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "8Wdoi9G7gloj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForQuestionAnswering\n",
        "\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n"
      ],
      "metadata": {
        "id": "2zwKu-e0gqiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "metadata": {
        "id": "sIMlRVYug5Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How many parameters does BERT-large have?\"\n",
        "answer_text = \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\"\n"
      ],
      "metadata": {
        "id": "vBL0l44Cg5FL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the tokenizer to the input text, treating them as a text-pair.\n",
        "input_ids = tokenizer.encode(question, answer_text)\n",
        "\n",
        "print('The input has a total of {:} tokens.'.format(len(input_ids)))"
      ],
      "metadata": {
        "id": "WFln3te0hHJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT only needs the token IDs, but for the purpose of inspecting the\n",
        "# tokenizer's behavior, let's also get the token strings and display them.\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "# For each token and its id...\n",
        "for token, id in zip(tokens, input_ids):\n",
        "\n",
        "    # If this is the [SEP] token, add some space around it to make it stand out.\n",
        "    if id == tokenizer.sep_token_id:\n",
        "        print('')\n",
        "\n",
        "    # Print the token string and its ID in two columns.\n",
        "    print('{:<12} {:>6,}'.format(token, id))\n",
        "\n",
        "    if id == tokenizer.sep_token_id:\n",
        "        print('')\n"
      ],
      "metadata": {
        "id": "TZ9mYrnohImQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Search the input_ids for the first instance of the `[SEP]` token.\n",
        "sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "\n",
        "# The number of segment A tokens includes the [SEP] token istelf.\n",
        "num_seg_a = sep_index + 1\n",
        "\n",
        "# The remainder are segment B.\n",
        "num_seg_b = len(input_ids) - num_seg_a\n",
        "\n",
        "# Construct the list of 0s and 1s.\n",
        "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "\n",
        "# There should be a segment_id for every input token.\n",
        "assert len(segment_ids) == len(input_ids)"
      ],
      "metadata": {
        "id": "zzLBY0wyhNIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run our example through the model.\n",
        "outputs = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
        "                             token_type_ids=torch.tensor([segment_ids]), # The segment IDs to differentiate question from answer_text\n",
        "                             return_dict=True)\n",
        "\n",
        "start_scores = outputs.start_logits\n",
        "end_scores = outputs.end_logits\n"
      ],
      "metadata": {
        "id": "l698IPSRhRC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the tokens with the highest `start` and `end` scores.\n",
        "answer_start = torch.argmax(start_scores)\n",
        "answer_end = torch.argmax(end_scores)\n",
        "\n",
        "# Combine the tokens in the answer and print it out.\n",
        "answer = ' '.join(tokens[answer_start:answer_end+1])\n",
        "\n",
        "print('Answer: \"' + answer + '\"')"
      ],
      "metadata": {
        "id": "DivKvrfshUQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start with the first token.\n",
        "answer = tokens[answer_start]\n",
        "\n",
        "# Select the remaining answer tokens and join them with whitespace.\n",
        "for i in range(answer_start + 1, answer_end + 1):\n",
        "\n",
        "    # If it's a subword token, then recombine it with the previous token.\n",
        "    if tokens[i][0:2] == '##':\n",
        "        answer += tokens[i][2:]\n",
        "\n",
        "    # Otherwise, add a space then the token.\n",
        "    else:\n",
        "        answer += ' ' + tokens[i]\n",
        "\n",
        "print('Answer: \"' + answer + '\"')"
      ],
      "metadata": {
        "id": "fUweGNbihboB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "#sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (16,8)"
      ],
      "metadata": {
        "id": "mqmBRi7qhdcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull the scores out of PyTorch Tensors and convert them to 1D numpy arrays.\n",
        "s_scores = start_scores.detach().numpy().flatten()\n",
        "e_scores = end_scores.detach().numpy().flatten()\n",
        "\n",
        "# We'll use the tokens as the x-axis labels. In order to do that, they all need\n",
        "# to be unique, so we'll add the token index to the end of each one.\n",
        "token_labels = []\n",
        "for (i, token) in enumerate(tokens):\n",
        "    token_labels.append('{:} - {:>2}'.format(token, i))\n"
      ],
      "metadata": {
        "id": "BryoKNgqhjG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a barplot showing the start word score for all of the tokens.\n",
        "ax = sns.barplot(x=token_labels, y=s_scores, ci=None)\n",
        "\n",
        "# Turn the xlabels vertical.\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
        "\n",
        "# Turn on the vertical grid to help align words to scores.\n",
        "ax.grid(True)\n",
        "\n",
        "plt.title('Start Word Scores')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lfvJIQObhoWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a barplot showing the end word score for all of the tokens.\n",
        "ax = sns.barplot(x=token_labels, y=e_scores, ci=None)\n",
        "\n",
        "# Turn the xlabels vertical.\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
        "\n",
        "# Turn on the vertical grid to help align words to scores.\n",
        "ax.grid(True)\n",
        "\n",
        "plt.title('End Word Scores')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ooaxvY15hsaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Store the tokens and scores in a DataFrame.\n",
        "# Each token will have two rows, one for its start score and one for its end\n",
        "# score. The \"marker\" column will differentiate them. A little wacky, I know.\n",
        "scores = []\n",
        "for (i, token_label) in enumerate(token_labels):\n",
        "\n",
        "    # Add the token's start score as one row.\n",
        "    scores.append({'token_label': token_label,\n",
        "                   'score': s_scores[i],\n",
        "                   'marker': 'start'})\n",
        "\n",
        "    # Add  the token's end score as another row.\n",
        "    scores.append({'token_label': token_label,\n",
        "                   'score': e_scores[i],\n",
        "                   'marker': 'end'})\n",
        "\n",
        "df = pd.DataFrame(scores)\n"
      ],
      "metadata": {
        "id": "z-bBbCMBhz0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw a grouped barplot to show start and end scores for each word.\n",
        "# The \"hue\" parameter is where we tell it which datapoints belong to which\n",
        "# of the two series.\n",
        "g = sns.catplot(x=\"token_label\", y=\"score\", hue=\"marker\", data=df,\n",
        "                kind=\"bar\", height=6, aspect=4)\n",
        "\n",
        "# Turn the xlabels vertical.\n",
        "g.set_xticklabels(g.ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
        "\n",
        "# Turn on the vertical grid to help align words to scores.\n",
        "g.ax.grid(True)\n"
      ],
      "metadata": {
        "id": "GoHpZmKkh5Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(question, answer_text):\n",
        "    '''\n",
        "    Takes a `question` string and an `answer_text` string (which contains the\n",
        "    answer), and identifies the words within the `answer_text` that are the\n",
        "    answer. Prints them out.\n",
        "    '''\n",
        "    # ======== Tokenize ========\n",
        "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
        "    input_ids = tokenizer.encode(question, answer_text)\n",
        "\n",
        "    # Report how long the input sequence is.\n",
        "    print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
        "\n",
        "    # ======== Set Segment IDs ========\n",
        "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
        "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "\n",
        "    # The number of segment A tokens includes the [SEP] token istelf.\n",
        "    num_seg_a = sep_index + 1\n",
        "\n",
        "    # The remainder are segment B.\n",
        "    num_seg_b = len(input_ids) - num_seg_a\n",
        "\n",
        "    # Construct the list of 0s and 1s.\n",
        "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "\n",
        "    # There should be a segment_id for every input token.\n",
        "    assert len(segment_ids) == len(input_ids)\n",
        "\n",
        "    # ======== Evaluate ========\n",
        "    # Run our example through the model.\n",
        "    outputs = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
        "                    token_type_ids=torch.tensor([segment_ids]), # The segment IDs to differentiate question from answer_text\n",
        "                    return_dict=True)\n",
        "\n",
        "    start_scores = outputs.start_logits\n",
        "    end_scores = outputs.end_logits\n",
        "\n",
        "    # ======== Reconstruct Answer ========\n",
        "    # Find the tokens with the highest `start` and `end` scores.\n",
        "    answer_start = torch.argmax(start_scores)\n",
        "    answer_end = torch.argmax(end_scores)\n",
        "\n",
        "    # Get the string versions of the input tokens.\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "    # Start with the first token.\n",
        "    answer = tokens[answer_start]\n",
        "\n",
        "    # Select the remaining answer tokens and join them with whitespace.\n",
        "    for i in range(answer_start + 1, answer_end + 1):\n",
        "\n",
        "        # If it's a subword token, then recombine it with the previous token.\n",
        "        if tokens[i][0:2] == '##':\n",
        "            answer += tokens[i][2:]\n",
        "\n",
        "        # Otherwise, add a space then the token.\n",
        "        else:\n",
        "            answer += ' ' + tokens[i]\n",
        "\n",
        "    print('Answer: \"' + answer + '\"')"
      ],
      "metadata": {
        "id": "hljlVNR9h9VS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "# Wrap text to 80 characters.\n",
        "wrapper = textwrap.TextWrapper(width=80)\n",
        "\n",
        "bert_abstract = \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\"\n",
        "\n",
        "print(wrapper.fill(bert_abstract))"
      ],
      "metadata": {
        "id": "vTeiVIh1lhkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is mean bert?\"\n",
        "\n",
        "answer_question(question, bert_abstract)"
      ],
      "metadata": {
        "id": "5SWPE8zxiP4w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}